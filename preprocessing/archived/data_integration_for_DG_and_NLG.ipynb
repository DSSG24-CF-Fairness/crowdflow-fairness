{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from IPython.display import display, HTML, Image, Markdown\n",
    "import geopandas\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dg_nlg_data_folder_id = 0\n",
    "folder_name = \"NY\"\n",
    "\n",
    "if folder_name == \"NY\":\n",
    "    dg_nlg_data_folder_name = \"new_york\" + str(dg_nlg_data_folder_id)\n",
    "elif folder_name == \"WA\":\n",
    "    dg_nlg_data_folder_name = \"washington\" + str(dg_nlg_data_folder_id)\n",
    "else:\n",
    "    raise ValueError(\"folder_name is not valid\")\n",
    "    \n",
    "raw_data_root_folder_path = os.path.join(\"../data\", folder_name)\n",
    "processed_data_root_folder_path = os.path.join(\"../processed_data\", folder_name)\n",
    "dg_nlg_data_root_folder_path = os.path.join(processed_data_root_folder_path, dg_nlg_data_folder_name)\n",
    "dg_nlg_data_DG_folder_path = os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\")\n",
    "dg_nlg_data_NLG_folder_path = os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\")\n",
    "\n",
    "\n",
    "os.makedirs(raw_data_root_folder_path, exist_ok=True)\n",
    "os.makedirs(processed_data_root_folder_path, exist_ok=True)\n",
    "os.makedirs(dg_nlg_data_root_folder_path, exist_ok=True)\n",
    "os.makedirs(dg_nlg_data_DG_folder_path, exist_ok=True)\n",
    "os.makedirs(dg_nlg_data_NLG_folder_path, exist_ok=True)"
   ],
   "id": "1382f4be591e473c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# copy file from src to tar\n",
    "def copy_file(src_folder_path, tar_folder_path, file_name):\n",
    "    src_file_path = os.path.join(src_folder_path, file_name)\n",
    "    tar_file_path = os.path.join(tar_folder_path, file_name)\n",
    "    shutil.copyfile(src_file_path, tar_file_path)\n",
    "    print(f\"Copy {file_name} file from {src_file_path} to {tar_file_path}\")\n",
    "\n",
    "\n",
    "copy_file(raw_data_root_folder_path, dg_nlg_data_root_folder_path, \"demographics.csv\")\n",
    "copy_file(raw_data_root_folder_path, dg_nlg_data_root_folder_path, \"features.csv\")\n",
    "copy_file(raw_data_root_folder_path, dg_nlg_data_root_folder_path, \"tessellation.geojson\")\n",
    "\n",
    "copy_file(processed_data_root_folder_path, dg_nlg_data_root_folder_path, \"test_region_index.csv\")\n",
    "copy_file(processed_data_root_folder_path, dg_nlg_data_root_folder_path, \"test_tile_geoids.csv\")\n",
    "copy_file(processed_data_root_folder_path, dg_nlg_data_root_folder_path, \"train_region_index.csv\")\n",
    "copy_file(processed_data_root_folder_path, dg_nlg_data_root_folder_path, \"train_tile_geoids.csv\")\n"
   ],
   "id": "5f8112e7192907ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge train and test flow data",
   "id": "22ac4351c8359c49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_flow_files = os.listdir(os.path.join(processed_data_root_folder_path, \"train\"))\n",
    "train_flow_files = [file for file in train_flow_files if \"flow\" in file]\n",
    "train_flow_files.sort()\n",
    "print(train_flow_files)\n",
    "\n",
    "# read processed_data_root_folder_path/train/train_flow.csv\n",
    "train_flow_file_path = os.path.join(processed_data_root_folder_path, \"train\", train_flow_files[dg_nlg_data_folder_id-1])\n",
    "print(train_flow_file_path)\n",
    "train_flow_df = pd.read_csv(train_flow_file_path)\n",
    "print(train_flow_df.shape)\n",
    "\n",
    "# read test \n",
    "test_flow_file_path = os.path.join(processed_data_root_folder_path, \"test\", \"test_flow.csv\")\n",
    "test_flow_df = pd.read_csv(test_flow_file_path)\n",
    "print(test_flow_df.shape)\n",
    "\n",
    "# combine train and test flow\n",
    "flow_df = pd.concat([train_flow_df, test_flow_df])\n",
    "print(flow_df.shape)\n",
    "flow_df.to_csv(os.path.join(dg_nlg_data_root_folder_path, \"flow.csv\"), index=False)\n"
   ],
   "id": "1e134af03a8d57e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2723330cbee1a0ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# convert to DG Data",
   "id": "38eb36c08cfe7005"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# test_tiles.csv\n",
    "test_tiles = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"test_region_index.csv\"))\n",
    "test_tiles.to_csv(os.path.join(dg_nlg_data_DG_folder_path, \"test_tiles.csv\"), header=False, index=False)\n",
    "print(\"Output -> test_tiles.csv, len is\", len(test_tiles))\n",
    "\n",
    "\n",
    "# train_tiles.csv\n",
    "train_tiles = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"train_region_index.csv\"))\n",
    "train_tiles.to_csv(os.path.join(dg_nlg_data_DG_folder_path, \"train_tiles.csv\"), header=False, index=False)\n",
    "print(\"Output -> train_tiles.csv, len is\", len(train_tiles))"
   ],
   "id": "ce64f71e9947b155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Original data\n",
    "flow = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"flow.csv\"))\n",
    "demographics = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"demographics.csv\"))\n",
    "tessellation = geopandas.read_file(os.path.join(dg_nlg_data_root_folder_path, \"tessellation.geojson\"))\n",
    "features = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"features.csv\"))\n",
    "\n",
    "\n",
    "# find the intersection of GEOIDs\n",
    "GEOIDs_flow = set()\n",
    "for i, row in flow.iterrows():\n",
    "    geoid_o = str(int(row[\"origin\"]))\n",
    "    geoid_d = str(int(row[\"destination\"]))\n",
    "    GEOIDs_flow.add(geoid_o)\n",
    "    GEOIDs_flow.add(geoid_d)\n",
    "\n",
    "GEOIDs_demographics = set()\n",
    "for i, row in demographics.iterrows():\n",
    "    geoid = str(int(row[\"geoid\"]))\n",
    "    GEOIDs_demographics.add(geoid)\n",
    "\n",
    "\n",
    "GEOIDs_features = set()\n",
    "for i, row in features.iterrows():\n",
    "    geoid = str(int(row[\"geoid\"]))\n",
    "    GEOIDs_features.add(geoid)\n",
    "\n",
    "GEOIDs_tessellation = set()\n",
    "for i, row in tessellation.iterrows():\n",
    "    geoid = str(int(row[\"GEOID\"]))\n",
    "    GEOIDs_tessellation.add(geoid)\n",
    "\n",
    "GEOIDs_intersected = GEOIDs_flow & GEOIDs_demographics & GEOIDs_features & GEOIDs_tessellation\n",
    "print(len(GEOIDs_intersected))"
   ],
   "id": "805778afbba41c91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# TO -> od2flow_new_york.csv.zip, res 138623 rows\n",
    "flows_oa = flow.rename(columns={\"origin\": \"residence\", \"destination\": \"workplace\", \"flow\": \"commuters\"})\n",
    "filtered_flows_oa = pd.DataFrame(columns=[\"residence\", \"workplace\", \"commuters\"])\n",
    "\n",
    "# Initialize an empty list to collect rows\n",
    "filtered_rows = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for i, row in tqdm(flows_oa.iterrows(), total=len(flows_oa), desc=\"-> flows_oa.csv.zip\"):\n",
    "    residence = str(int(row[\"residence\"]))\n",
    "    workplace = str(int(row[\"workplace\"]))\n",
    "    \n",
    "    # Check if both residence and workplace are in GEOIDs_intersected\n",
    "    if residence in GEOIDs_intersected and workplace in GEOIDs_intersected:\n",
    "        # Append the row data as a tuple to the list\n",
    "        filtered_rows.append([residence, workplace, row[\"commuters\"]])\n",
    "\n",
    "# Convert the list of rows to a DataFrame at once\n",
    "filtered_flows_oa = pd.DataFrame(filtered_rows, columns=[\"residence\", \"workplace\", \"commuters\"])\n",
    "\n",
    "filtered_flows_oa_grouped = filtered_flows_oa.groupby(['residence', 'workplace'], as_index=False).sum()\n",
    "filtered_flows_oa_grouped = filtered_flows_oa_grouped[[\"residence\", \"workplace\", \"commuters\"]]\n",
    "print(\"Output -> flows_oa.csv.zip, len is\", len(filtered_flows_oa_grouped))\n",
    "filtered_flows_oa_grouped.to_csv(os.path.join(dg_nlg_data_DG_folder_path, \"flows_oa.csv.zip\"), index=False)\n",
    "filtered_flows_oa_grouped"
   ],
   "id": "6b336ba6b91dc386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO -> od2flow.pkl, res 138623 rows\n",
    "od2flow = {(str(int(row['residence'])), str(int(row['workplace']))): row['commuters'] for _, row in filtered_flows_oa_grouped.iterrows()}\n",
    "\n",
    "with open(os.path.join(dg_nlg_data_DG_folder_path, 'od2flow.pkl'), 'wb') as f:\n",
    "    pickle.dump(od2flow, f)\n",
    "print(\"Output -> od2flow.pkl, len is\", len(od2flow))\n",
    "od2flow"
   ],
   "id": "df68dd613be39ef3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO -> oa2centroid.pkl, res 1429 rows\n",
    "oa2centroid = dict()\n",
    "# Extracting lng and lat of each GEOID into a dictionary\n",
    "\n",
    "for i, row in tqdm(tessellation.iterrows(), total=len(tessellation), desc= \"-> oa2centroid.pkl\"):\n",
    "    geoid = str(int(row[\"GEOID\"]))\n",
    "    if geoid in GEOIDs_intersected:\n",
    "        oa2centroid[geoid] = [row[\"lng\"], row[\"lat\"]]\n",
    "with open(os.path.join(dg_nlg_data_DG_folder_path, \"oa2centroid.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(oa2centroid, f)\n",
    "print(\"Output -> oa2centroid.pkl, len is\", len(oa2centroid))\n",
    "oa2centroid"
   ],
   "id": "5a99eb732ca41853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO -> oa2features.pkl\n",
    "oa2features = dict()\n",
    "\n",
    "# for i, row in features.iterrows():\n",
    "for i, row in tqdm(features.iterrows(), total=len(features), desc= \"-> oa2features.pkl\"):\n",
    "    geoid = str(int(row[\"geoid\"]))\n",
    "\n",
    "    if geoid in GEOIDs_intersected:\n",
    "        # Select all columns except the first one\n",
    "        feature = row.iloc[1:].tolist()\n",
    "\n",
    "        oa2features[geoid] = feature\n",
    "\n",
    "\n",
    "        oa2features[geoid] = feature\n",
    "\n",
    "with open(os.path.join(dg_nlg_data_DG_folder_path, \"oa2features.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(oa2features, f)\n",
    "print(\"Output -> oa2features.pkl, len is\", len(oa2features))\n",
    "oa2features"
   ],
   "id": "b253aa14e926a27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b04902d18a03f256",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO -> oa_gdf.csv.gz\n",
    "oa_gdf = []\n",
    "# for i, geo_code in enumerate(GEOIDs_intersected):\n",
    "for i, geo_code in tqdm(enumerate(GEOIDs_intersected), total=len(GEOIDs_intersected), desc= \"-> oa_gdf.csv.gz\"):\n",
    "    # Default values\n",
    "    centroid = None\n",
    "    area_km2 = None\n",
    "\n",
    "    # Check if geo_code exists in the tessellation DataFrame\n",
    "    tessellation_match = tessellation[tessellation['GEOID'] == geo_code]\n",
    "    if not tessellation_match.empty:\n",
    "        lng = tessellation_match.iloc[0]['lng']\n",
    "        lat = tessellation_match.iloc[0]['lat']\n",
    "        centroid = [lng, lat]  # Store centroid as a tuple\n",
    "    else:\n",
    "        print(f\"Tessellation data not found for GEOID {geo_code}\")\n",
    "\n",
    "    # Check if geo_code exists in the demographics DataFrame\n",
    "    demographics_match = demographics[demographics['geoid'] == int(geo_code)]\n",
    "    if not demographics_match.empty:\n",
    "        area_sqmi = demographics_match.iloc[0]['AREA_SQMI']\n",
    "        area_km2 = area_sqmi * 2.58999  # Convert sq mi to kmÂ²\n",
    "    else:\n",
    "        print(f\"Demographics data not found for GEOID {geo_code}\")\n",
    "\n",
    "    # Append the data as a dictionary\n",
    "    oa_gdf.append({\n",
    "        \"Unnamed: 0\": i,  # Index value\n",
    "        \"geo_code\": geo_code,\n",
    "        \"centroid\": centroid,\n",
    "        \"area_km2\": area_km2\n",
    "    })\n",
    "\n",
    "# Create the final DataFrame from the collected data\n",
    "oa_gdf = pd.DataFrame(oa_gdf)\n",
    "oa_gdf.to_csv(os.path.join(dg_nlg_data_DG_folder_path, \"oa_gdf.csv.gz\"), index=False)\n",
    "print(\"Output -> oa_gdf.csv.gz, len is\", len(oa_gdf))\n",
    "oa_gdf"
   ],
   "id": "1e644f8891e8393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3a64ae38ec6a7b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO -> tileid2oa2handmade_features.json\n",
    "features_mapping = {}\n",
    "for index, row in tqdm(features.iterrows(), total=len(features), desc=\"featrues -> tileid2oa2handmade_features.json\"):\n",
    "    geoid = int(row[\"geoid\"])\n",
    "\n",
    "    # Proceed only if geoid is in the intersected set\n",
    "    if str(geoid) in GEOIDs_intersected:\n",
    "        feature_data = row.drop(\"geoid\").to_dict()\n",
    "\n",
    "        # Convert each feature's value to a list containing that value\n",
    "        feature_data = {key: [value] for key, value in feature_data.items()}\n",
    "        features_mapping[str(geoid)] = feature_data\n",
    "\n",
    "for geoid in GEOIDs_intersected:\n",
    "    if str(geoid) not in features_mapping:\n",
    "        features_mapping[str(geoid)] = {}\n",
    "\n",
    "print(\"features_mapping size\", len(features_mapping))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "region_dict = {}\n",
    "\n",
    "\n",
    "with open(os.path.join(dg_nlg_data_root_folder_path, \"train_region_index.csv\"), mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        region_dict[int(row[0])] = {}\n",
    "        \n",
    "        \n",
    "with open(os.path.join(dg_nlg_data_root_folder_path, \"test_region_index.csv\"), mode='r') as file:\n",
    "# the test_region_index is a csv file has one column called region_index, store all region_index to a list except for the header\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        region_dict[int(row[0])] = {}\n",
    "\n",
    "# sort keys in region_dict\n",
    "region_dict = dict(sorted(region_dict.items()))\n",
    "\n",
    "\n",
    "valid_geoids = set()\n",
    "invalid_geoids = set()\n",
    "with open(os.path.join(dg_nlg_data_root_folder_path, \"test_tile_geoids.csv\"), mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        region_index = int(row[0])  # Convert region_index to integer\n",
    "        geoids = row[1].split(',')  # Split the geoids into a list\n",
    "        used_geoids = []\n",
    "        for geoid in geoids:\n",
    "            geoid = geoid.strip()\n",
    "            if geoid in GEOIDs_intersected:\n",
    "                valid_geoids.add(geoid)\n",
    "                used_geoids.append(geoid)\n",
    "            else:\n",
    "                invalid_geoids.add(geoid)\n",
    "        region_dict[region_index] = used_geoids  # Store in the dictionary\n",
    "\n",
    "with open(os.path.join(dg_nlg_data_root_folder_path,  \"train_tile_geoids.csv\"), mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        region_index = int(row[0])  # Convert region_index to integer\n",
    "        geoids = row[1].split(',')  # Split the geoids into a list\n",
    "        used_geoids = []\n",
    "        for geoid in geoids:\n",
    "            geoid = geoid.strip()\n",
    "            if geoid in GEOIDs_intersected:\n",
    "                valid_geoids.add(geoid)\n",
    "                used_geoids.append(geoid)\n",
    "            else:\n",
    "                invalid_geoids.add(geoid)\n",
    "        region_dict[region_index] = used_geoids\n",
    "\n",
    "tileid2oa2handmade_features = {}\n",
    "used_geoids = set()\n",
    "\n",
    "for region_id, zone_ids in region_dict.items():\n",
    "    tileid2oa2handmade_features[region_id] = {}\n",
    "    for zone_id in zone_ids:\n",
    "        zone_features = features_mapping.get(zone_id, {})\n",
    "        tileid2oa2handmade_features[region_id][zone_id] = zone_features\n",
    "        used_geoids.add(zone_id)\n",
    "# write to processed/tileid2oa2handmade_features.json\n",
    "with open(os.path.join(dg_nlg_data_DG_folder_path, \"tileid2oa2handmade_features.json\"), \"w\") as f:\n",
    "    json.dump(tileid2oa2handmade_features, f)\n",
    "    \n",
    "print(\"Output -> tileid2oa2handmade_features.json, used geoid counts is\", len(used_geoids))\n",
    "print(\"Output -> tileid2oa2handmade_features.json, total used tile counts is\", len(tileid2oa2handmade_features))\n",
    "\n",
    "tileid2oa2handmade_features\n"
   ],
   "id": "8d8d902950043dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# convert to NLG Data",
   "id": "1c1c630004eb7027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "tessellation_data = gpd.read_file(os.path.join(dg_nlg_data_root_folder_path, \"tessellation.geojson\"))\n",
    "tessellation_data.to_file(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"tessellation.geojson\"), driver=\"GeoJSON\")\n",
    "\n",
    "# clean handmade features\n",
    "with open(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"tileid2oa2handmade_features.json\"), \"r\") as f:\n",
    "    tileid2oa2handmade_features = json.load(f)\n",
    "\n",
    "geoid_to_population = {}\n",
    "for tileid in tileid2oa2handmade_features:\n",
    "    for oa in tileid2oa2handmade_features[tileid]:\n",
    "        geoid_to_population[oa] = tileid2oa2handmade_features[tileid][oa]['total_population']\n",
    "        tileid2oa2handmade_features[tileid][oa] = {'total_population': geoid_to_population[oa]}\n",
    "with open(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"tileid2oa2handmade_features.json\"), \"w\") as f:\n",
    "    json.dump(tileid2oa2handmade_features, f)\n",
    "\n",
    "\n",
    "\n",
    "# clean features\n",
    "oa2features = pickle.load(open(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"oa2features.pkl\"), \"rb\"))\n",
    "\n",
    "for oa in oa2features:\n",
    "    oa2features[oa] = geoid_to_population[oa]\n",
    "pickle.dump(oa2features, open(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"oa2features.pkl\"), \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# copy rest of the files to processed_NLG\n",
    "flows_oa = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"flows_oa.csv.zip\"))\n",
    "flows_oa.to_csv(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"flows_oa.csv.zip\"), index=False)\n",
    "\n",
    "od2flow = pickle.load(open(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"od2flow.pkl\"), \"rb\"))\n",
    "pickle.dump(od2flow, open(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"od2flow.pkl\"), \"wb\"))\n",
    "\n",
    "oa2centroid = pickle.load(open(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"oa2centroid.pkl\"), \"rb\"))\n",
    "pickle.dump(oa2centroid, open(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"oa2centroid.pkl\"), \"wb\"))\n",
    "\n",
    "oa_gdf = pd.read_csv(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"oa_gdf.csv.gz\"))\n",
    "oa_gdf.to_csv(os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"oa_gdf.csv.gz\"), index=False)\n",
    "\n",
    "\n",
    "# copy test_tiles.csv and train_tiles.csv to processed_NLG\n",
    "shutil.copy(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"test_tiles.csv\"), os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"test_tiles.csv\"))\n",
    "shutil.copy(os.path.join(dg_nlg_data_root_folder_path, \"processed_DG\", \"train_tiles.csv\"), os.path.join(dg_nlg_data_root_folder_path, \"processed_NLG\", \"train_tiles.csv\"))\n"
   ],
   "id": "d235d495400ad029",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
