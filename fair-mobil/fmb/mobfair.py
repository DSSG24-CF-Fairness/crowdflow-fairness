import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class FairMob:

    def __init__(self):
        pass

    def KLDiva(self, arr1, arr2):
        """
        This function takes in two arrays. The two arrays are assumed to be of the same length.

        This function uses the two arrays and normalizes the contents' distribution. To solve
        the issue of calculating the entropy of 0 values, the function will add a miniscule number,
        1 x 10^-9 to all the values in the array. The function will calculate both the Kullback-Leibler
        divergence and the Jensen-Shannon divergence.  
        """
        epsilon = 1e-9
        arr1 = arr1 + epsilon
        arr2 = arr2 + epsilon
        arr1norm = arr1/arr1.sum()
        arr2norm = arr2/arr2.sum()
        m = 0.5*(arr1 + arr2)
        js_div = 0.5 * stats.entropy(arr1,m) + 0.5 * stats.entropy(arr2,m)
        kldiv = stats.entropy(arr1norm,arr2norm)
        return js_div, kldiv
    
    def equalArray (self,arr1, arr2):
        """
        This function takes in two arrays and upsamples the smaller array to match the legnth of the larger array.

        This function uses the two arrays and will compare their lengths to find the smaller array. The smaller array
        will be upsampled using pre-existing values. The upsampling is done through a random selection.
        """
        len_arr1 = len(arr1)
        len_arr2 = len(arr2)

        if(len_arr1 < len_arr2):
            shorter_arr, longer_array_len = arr1, len_arr2
        elif (len_arr2 < len_arr1):
            shorter_arr, longer_array_len = arr2, len_arr1
        else:
            return arr1, arr2
        
        bootstrap = np.random.choice(len(shorter_arr),longer_array_len, replace=True)
        equalArr = shorter_arr[bootstrap]

        if len_arr1 < len_arr2:
            return equalArr, arr2 
        else:
            return arr1, equalArr
        
    def runFairness(self,df, measurementdf, percentile, boolSaveDFs = False, realflow="flow", modelFlow="mFlow", measurementOrigin = "FIPS", measurement="RPL_THEMES", measurement2 = None, measurement3 = None, measurement4 = None, measurement5 = None,
                        mainOrigin = "origin", mainDestination = "destination", onOrigin = True, bidirectional = False):
        """
        The Necessary parameters are
            df, measurementdf, percentile, bidirectional.
            The rest of the parameters are to tweak it to fit the names of your Pandas Columns.
        """
        
        """
        Function Parameters:
            df: Pandas DataFrame with origin, destination, flow, and a model flow column.
            measurementdf: Pandas DataFrame with 1-5 measurement columns and a main mapping column to origin or destination.
            percentile: An int that represents the top and bottom percent that you want to examine.
            boolSaveDFs: boolean that determines whether or not you want to save the various csvs and txt files generated by merging.
            realflow: string that is the title of the real flows column in the df DataFrame.
            modelFlow: string that is the title of the synthetic flows column in the df DataFrame.
            measurementOrigin: string that is the title of the origin/destination mapping column in the measurementdf DataFrame.
            measurement: string that is the title of the first measurement of the measurementdf DataFrame. 
            measurement2: optional string that is the title of the second measurement of the measurementdf DataFrame.
            measurement3: optional string that is the title of the third measurement of the measurementdf DataFrame.
            measurement4: optional string that is the title of the fourth measurement of the measurementdf DataFrame.
            measurement5: optional string that is the title of the fifth measurement of the measurementdf DataFrame.
            mainOrigin: string that is the title of the Origin column in th df DataFrame.
            mainDestination: string that is the title of the Destination column in the df DataFrame.
            onOrigin: boolean value that determines whether directional_Fairness will look at Origin or Destination for analysis.
                    True will look at Origin, and False will look at destination.
            bidirectional: boolean value that determines if the function will run directional or bidirectional analysis. 
                        True will complete bidirectional fairness, and False will complete directional fairness.

        This function will generate an interactive graph as well as print out the necessary information to perform fairness analysis.
        The interactive plot will have up to 5 measurements and look at the percentiles specified by the user. 

        """
        
        if(bidirectional):
            self.bidirectional_fairness(df,measurementdf, percentile,boolSaveDFs,realflow,modelFlow,measurementOrigin,measurement,measurement2,measurement3,measurement4,measurement5,mainOrigin,mainDestination)
        else:
            self.directional_Fairness(df,measurementdf, percentile,boolSaveDFs, realflow,modelFlow,measurementOrigin, measurement,measurement2, measurement3,measurement4,measurement5,mainOrigin,mainDestination,onOrigin)
        
        
    def directional_Fairness(self,df, measurementdf, percentile, boolSaveDFs = False, realflow="flow", modelFlow="mFlow", measurementOrigin = "FIPS", measurement="RPL_THEMES", measurement2 = None, measurement3 = None, measurement4 = None, measurement5 = None, mainOrigin = "origin", mainDestination = "destination", onOrigin = True):
        """
        Function Parameters:
            df: Pandas DataFrame with origin, destination, flow, and a model flow column.
            measurementdf: Pandas DataFrame with 1-5 measurement columns and a main mapping column to origin or destination.
            percentile: An int that represents the top and bottom percent that you want to examine.
            boolSaveDFs: boolean that determines whether or not you want to save the various csvs and txt files generated by merging.
            realflow: string that is the title of the real flows column in the df DataFrame.
            modelFlow: string that is the title of the synthetic flows column in the df DataFrame.
            measurementOrigin: string that is the title of the origin/destination mapping column in the measurementdf DataFrame.
            measurement: string that is the title of the first measurement of the measurementdf DataFrame. 
            measurement2: optional string that is the title of the second measurement of the measurementdf DataFrame.
            measurement3: optional string that is the title of the third measurement of the measurementdf DataFrame.
            measurement4: optional string that is the title of the fourth measurement of the measurementdf DataFrame.
            measurement5: optional string that is the title of the fifth measurement of the measurementdf DataFrame.
            mainOrigin: string that is the title of the Origin column in th df DataFrame.
            mainDestination: string that is the title of the Destination column in the df DataFrame.
            onOrigin: boolean value that determines whether directional_Fairness will look at Origin or Destination for analysis.
                    True will look at Origin, and False will look at destination.

        This function will generate an interactive graph as well as print out the necessary information to perform fairness analysis.
        The interactive plot will have up to 5 measurements and look at the percentiles specified by the user. 
        
        """
        # Calculates CPC and creates a DataFrame
        count = 1
        df['CPC'] = 2.0 * np.minimum(df[realflow], df[modelFlow]) / ((df[realflow]) + (df[modelFlow]))
        df.loc[(df[modelFlow] == 0.00) & (df[realflow] == 0.000), 'CPC'] = 1
        df1 = df.copy()
        meas1df = measurementdf[[measurementOrigin,measurement]].copy()
        # Depending on Origin or Destination, Creates Dataframe with first measurement as metric
        if(onOrigin): 
            meas1df.rename(columns={measurementOrigin:mainOrigin,measurement:'oMeasure'},inplace=True)
            mergDF = pd.merge(df1,meas1df,on=mainOrigin)
            mergDF = mergDF[mergDF['oMeasure'] >= 0]
            mergDF = mergDF[~mergDF['CPC'].isna()]
            if(boolSaveDFs):
                mergDF.to_csv('normFairnessO.csv')
        else:
            meas1df.rename(columns={measurementOrigin:mainDestination,measurement:'dMeasure'},inplace=True)
            mergDF = pd.merge(df1,meas1df,on=mainDestination)
            mergDF = mergDF[mergDF['dMeasure'] >= 0]
            mergDF = mergDF[~mergDF['CPC'].isna()]
            if(boolSaveDFs):
                mergDF.to_csv('normFairnessDE.csv')
        
        #If second measurement exists, creates DataFrame with 2nd measurement as metric
        #Filters for the user specified percentile and can save the results as a csv and a txt file.
        if(measurement2 is not None):
            df2 = df.copy()
            meas2df = measurementdf[[measurementOrigin,measurement2]].copy()
            if(onOrigin): 
                meas2df.rename(columns={measurementOrigin:mainOrigin,measurement2:'oMeasure'},inplace=True)
                mergDF2 = pd.merge(df2,meas2df,on=mainOrigin)
                mergDF2 = mergDF2[mergDF2['oMeasure'] >= 0]
                mergDF2 = mergDF2[~mergDF2['CPC'].isna()]
                orgTop2 = np.percentile(mergDF2['oMeasure'],percentile)
                orgBot2 = np.percentile(mergDF2['oMeasure'],100-percentile)
                topdf2 = mergDF2[mergDF2['oMeasure'] <= orgTop2]
                botdf2 = mergDF2[mergDF2['oMeasure'] >= orgBot2]
            else:
                meas2df.rename(columns={measurementOrigin:mainDestination,measurement2:'dMeasure'},inplace=True)
                mergDF2 = pd.merge(df2,meas2df,on=mainDestination)
                mergDF2 = mergDF2[mergDF2['dMeasure'] >= 0]
                mergDF2 = mergDF2[~mergDF2['CPC'].isna()]
                destTop2 = np.percentile(mergDF2['dMeasure'],percentile)
                destBot2 = np.percentile(mergDF2['dMeasure'],100-percentile)
                topdf2 = mergDF2[mergDF2['dMeasure'] <= destTop2]
                botdf2 = mergDF2[mergDF2['dMeasure'] >= destBot2]
            
            

            arrtop2 = topdf2['CPC'].values
            arrbot2 = botdf2['CPC'].values
            if(boolSaveDFs):
                topdf2.to_csv("topdf2.csv",index=False)
                botdf2.to_csv("botdf2.csv",index=False)
                np.savetxt('arrtop2.txt',arrtop2)
                np.savetxt('arrbot2.txt',arrbot2)
            # Upsamples if necessary for distribution calculations later
            arrtop2,arrbot2 = self.equalArray(arrtop2,arrbot2)
            count+=1
        
        #If third measurement exists, creates DataFrame with 3rd measurement as metric
        #Filters for the user specified percentile and can save the results as a csv and a txt file.
        if(measurement3 is not None):
            df3 = df.copy()
            meas3df = measurementdf[[measurementOrigin,measurement3]].copy()
            if(onOrigin): 
                meas3df.rename(columns={measurementOrigin:mainOrigin,measurement3:'oMeasure'},inplace=True)
                mergDF3 = pd.merge(df3,meas3df,on=mainOrigin)
                mergDF3 = mergDF3[mergDF3['oMeasure'] >= 0]
                mergDF3 = mergDF3[~mergDF3['CPC'].isna()]
                orgTop3 = np.percentile(mergDF3['oMeasure'],percentile)
                orgBot3 = np.percentile(mergDF3['oMeasure'],100-percentile)
                topdf3 = mergDF3[mergDF3['oMeasure'] <= orgTop3]
                botdf3 = mergDF3[mergDF3['oMeasure'] >= orgBot3]
            else:
                meas3df.rename(columns={measurementOrigin:mainDestination,measurement3:'dMeasure'},inplace=True)
                mergDF3 = pd.merge(df3,meas3df,on=mainDestination)
                mergDF3 = mergDF3[mergDF3['dMeasure'] >= 0]
                mergDF3 = mergDF3[~mergDF3['CPC'].isna()]
                destTop3 = np.percentile(mergDF3['dMeasure'],percentile)
                destBot3 = np.percentile(mergDF3['dMeasure'],100-percentile)
                topdf3 = mergDF3[mergDF3['dMeasure'] <= destTop3]
                botdf3 = mergDF3[mergDF3['dMeasure'] >= destBot3]
            
            

            arrtop3 = topdf3['CPC'].values
            arrbot3 = botdf3['CPC'].values
            if(boolSaveDFs):
                np.savetxt('arrtop3.txt',arrtop3)
                np.savetxt('arrbot3.txt',arrbot3)
                topdf3.to_csv("topdf3.csv",index=False)
                botdf3.to_csv("botdf3.csv",index=False)
            #Upsamples if necessary for distribution calculation
            arrtop3,arrbot3 = self.equalArray(arrtop3,arrbot3)
            count+=1

        #If fourth measurement exists, creates DataFrame with fourth measurement as metric
        #Filters for the user specified percentile and can save the results as a csv and a txt file.
        if(measurement4 is not None):
            df4 = df.copy()
            meas4df = measurementdf[[measurementOrigin,measurement4]].copy()
            if(onOrigin): 
                meas4df.rename(columns={measurementOrigin:mainOrigin,measurement4:'oMeasure'},inplace=True)
                mergDF4 = pd.merge(df4,meas4df,on=mainOrigin)
                mergDF4 = mergDF4[mergDF4['oMeasure'] >= 0]
                mergDF4 = mergDF4[~mergDF4['CPC'].isna()]
                orgTop4 = np.percentile(mergDF4['oMeasure'],percentile)
                orgBot4 = np.percentile(mergDF4['oMeasure'],100-percentile)
                topdf4 = mergDF4[mergDF4['oMeasure'] <= orgTop4]
                botdf4 = mergDF4[mergDF4['oMeasure'] >= orgBot4]
            else:
                meas4df.rename(columns={measurementOrigin:mainDestination,measurement4:'dMeasure'},inplace=True)
                mergDF4 = pd.merge(df4,meas4df,on=mainDestination)
                mergDF4 = mergDF4[mergDF4['dMeasure'] >= 0]
                mergDF4 = mergDF4[~mergDF4['CPC'].isna()]
                destTop4 = np.percentile(mergDF4['dMeasure'],percentile)
                destBot4 = np.percentile(mergDF4['dMeasure'],100-percentile)
                topdf4 = mergDF4[mergDF4['dMeasure'] <= destTop4]
                botdf4 = mergDF4[mergDF4['dMeasure'] >= destBot4]
            
            
            arrtop4 = topdf4['CPC'].values
            arrbot4 = botdf4['CPC'].values
            if(boolSaveDFs):
                topdf4.to_csv("topdf4.csv",index=False)
                botdf4.to_csv("botdf4.csv",index=False)
                np.savetxt('arrtop4.txt',arrtop4)
                np.savetxt('arrbot4.txt',arrbot4)
            arrtop4,arrbot4 = self.equalArray(arrtop4,arrbot4)
            count+=1
        
        #If fifth measurement exists, creates DataFrame with fifth measurement as metric
        #Filters for the user specified percentile and can save the results as a csv and a txt file.
        if(measurement5 is not None):
            df5 = df.copy()
            meas5df = measurementdf[[measurementOrigin,measurement5]].copy()
            if(onOrigin): 
                meas5df.rename(columns={measurementOrigin:mainOrigin,measurement5:'oMeasure'},inplace=True)
                mergDF5 = pd.merge(df5,meas5df,on=mainOrigin)
                mergDF5 = mergDF5[mergDF5['oMeasure'] >= 0]
                mergDF5 = mergDF5[~mergDF5['CPC'].isna()]
                orgTop5 = np.percentile(mergDF5['oMeasure'],percentile)
                orgBot5 = np.percentile(mergDF5['oMeasure'],100-percentile)
                topdf5 = mergDF5[mergDF5['oMeasure'] <= orgTop5]
                botdf5 = mergDF5[mergDF5['oMeasure'] >= orgBot5]
            else:
                meas5df.rename(columns={measurementOrigin:mainDestination,measurement5:'dMeasure'},inplace=True)
                mergDF5 = pd.merge(df5,meas5df,on=mainDestination)
                mergDF5 = mergDF5[mergDF5['dMeasure'] >= 0]
                mergDF5 = mergDF5[~mergDF5['CPC'].isna()]
                destTop5 = np.percentile(mergDF5['dMeasure'],percentile)
                destBot5 = np.percentile(mergDF5['dMeasure'],100-percentile)
                topdf5 = mergDF5[mergDF5['dMeasure'] <= destTop5]
                botdf5 = mergDF5[mergDF5['dMeasure'] >= destBot5]
            
            

            arrtop5 = topdf5['CPC'].values
            arrbot5 = botdf5['CPC'].values
            if(boolSaveDFs):
                np.savetxt('arrtop5.txt',arrtop5)
                np.savetxt('arrbot5.txt',arrbot5)
                topdf5.to_csv("topdf5.csv",index=False)
                botdf5.to_csv("botdf5.csv",index=False)
            arrtop5,arrbot5 = self.equalArray(arrtop5,arrbot5)
            count+=1

        #Finishes calculating for first measurement.
        #Filters for the user specified percentile and can save the results as a csv and a txt file.
        if(onOrigin):
            orgTop = np.percentile(mergDF['oMeasure'],percentile)
            orgBot = np.percentile(mergDF['oMeasure'],100-percentile)
            topdf = mergDF[mergDF['oMeasure'] <= orgTop]
            botdf = mergDF[mergDF['oMeasure'] >= orgBot]
        else:
            destTop = np.percentile(mergDF['dMeasure'],percentile)
            destBot = np.percentile(mergDF['dMeasure'],100-percentile)
            topdf = mergDF[mergDF['dMeasure'] <= destTop] 
            botdf = mergDF[mergDF['dMeasure'] >= destBot]
        
        
        arrtop1 = topdf['CPC'].values
        arrbot1 = botdf['CPC'].values
        if(boolSaveDFs):
            np.savetxt('arrtop1.txt',arrtop1)
            np.savetxt('arrbot1.txt',arrbot1)
            topdf.to_csv("topdf.csv",index=False)
            botdf.to_csv("botdf.csv",index=False)
        arrtop1,arrbot1 = self.equalArray(arrtop1,arrbot1)

        #Calculates the Jensen-Shannon Divergence and KL Divergence for first measurement
        js_div,kldiv = self.KLDiva(arrtop1,arrbot1)
        print("Jensen-Shannon Divergence: ", js_div)
        print("KL Divergence: ", kldiv)
        
        fig = make_subplots(rows = 1, cols = count)

        fig.update_layout(
            title = f"Top and Bottom {percentile} Percent for " + measurement
        )
        
        fig.add_trace(go.Box(name = "Richer Areas " + measurement, y=arrtop1, boxmean='sd'),row=1,col=1)
        fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement, y=arrbot1, boxmean='sd'),row=1,col=1)
        drawn = 1
        if(measurement2 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement2, y=arrtop2, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement2, y=arrbot2, boxmean='sd'),row=1,col=drawn)
            title = measurement2 
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div2,kldiv2 = self.KLDiva(arrtop2,arrbot2)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div2))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv2))

        if(measurement3 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement3, y=arrtop3, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement3, y=arrbot3, boxmean='sd'),row=1,col=drawn)
            title = measurement3
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div3,kldiv3 = self.KLDiva(arrtop3,arrbot3)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div3))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv3))

        if(measurement4 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement4, y=arrtop4, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement4, y=arrbot4, boxmean='sd'),row=1,col=drawn)
            title = measurement4
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div4,kldiv4 = self.KLDiva(arrtop4,arrbot4)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div4))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv4))
        
        if(measurement5 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement5, y=arrtop5, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement5, y=arrbot5, boxmean='sd'),row=1,col=drawn)
            title = measurement5 
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div5,kldiv5 = self.KLDiva(arrtop5,arrbot5)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div5))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv5))

        title = measurement 
        fig.update_xaxes(title_text=title, row=1, col=1)
        

        fig.show()

        #Prints other helpful value
        print("SVI Low End CPC Mean:", np.mean(topdf['CPC']))
        print("SVI Low End CPC Standard Deviation:", np.std(topdf['CPC']))
        print("SVI High End CPC Mean:", np.mean(botdf['CPC']))
        print("SVI High End CPC Standard Deviation:", np.std(botdf['CPC']))
        tstat, pval = stats.ttest_ind(topdf['CPC'],botdf['CPC'])
        print("T-statistic: ", tstat)
        print("p-value: ", pval)
            
        plt.show()




    def bidirectional_fairness(self,df, measurementdf, percentile, boolSaveDFs = False, realflow="flow", modelFlow="mFlow", measurementOrigin = "FIPS", measurement="RPL_THEMES", measurement2 = None, measurement3 = None, measurement4 = None, measurement5 = None, mainOrigin = "origin", mainDestination = "destination"):
        """
        Function Parameters:
            df: Pandas DataFrame with origin, destination, flow, and a model flow column.
            measurementdf: Pandas DataFrame with 1-5 measurement columns and a main mapping column to origin or destination.
            percentile: An int that represents the top and bottom percent that you want to examine.
            boolSaveDFs: boolean that determines whether or not you want to save the various csvs and txt files generated by merging.
            realflow: string that is the title of the real flows column in the df DataFrame.
            modelFlow: string that is the title of the synthetic flows column in the df DataFrame.
            measurementOrigin: string that is the title of the origin/destination mapping column in the measurementdf DataFrame.
            measurement: string that is the title of the first measurement of the measurementdf DataFrame. 
            measurement2: optional string that is the title of the second measurement of the measurementdf DataFrame.
            measurement3: optional string that is the title of the third measurement of the measurementdf DataFrame.
            measurement4: optional string that is the title of the fourth measurement of the measurementdf DataFrame.
            measurement5: optional string that is the title of the fifth measurement of the measurementdf DataFrame.
            mainOrigin: string that is the title of the Origin column in th df DataFrame.
            mainDestination: string that is the title of the Destination column in the df DataFrame.

        This function will generate an interactive graph as well as print out the necessary information to perform fairness analysis.
        The interactive plot will have up to 5 measurements and look at the percentiles specified by the user. bidirectional_fairness 
        will look at origin to destination pairs and analyze for points where both origin and destination are in the user specified percentile. 

        """
        
        # Calculates CPC 
        count = 1
        df['CPC'] = 2.0 * np.minimum(df[realflow], df[modelFlow]) / ((df[realflow]) + (df[modelFlow]))
        df.loc[(df[modelFlow] == 0.00) & (df[realflow] == 0.000), 'CPC'] = 1
        print("CPC Average: ", np.mean(df['CPC']))

        #Performs calculation for first measurement
        df1 = df.copy()
        meas1df = measurementdf[[measurementOrigin,measurement]].copy()
        meas1df.rename(columns={measurementOrigin:mainOrigin,measurement:'oMeasure'}, inplace=True)
        df1 = pd.merge(df1,meas1df, on=mainOrigin)
        meas1df.rename(columns={mainOrigin:mainDestination,'oMeasure':'dMeasure'}, inplace=True)
        df1 = pd.merge(df1,meas1df, on=mainDestination)
        df1.to_csv("hilowflows.csv",index=False)
        df1 = df1[df1['oMeasure'] >= 0]
        df1 = df1[df1['dMeasure'] >= 0]
        df1 = df1[(~df1['CPC'].isna())]

        if(measurement2 is not None):
            df2 = df.copy()
            meas2df = measurementdf[[measurementOrigin,measurement2]].copy()
            meas2df.rename(columns={measurementOrigin:mainOrigin, measurement2:'oMeasure'},inplace=True)
            df2 = pd.merge(df2,meas2df,on=mainOrigin)
            meas2df.rename(columns={mainOrigin:mainDestination,'oMeasure':'dMeasure'}, inplace=True)
            df2 = pd.merge(df2,meas2df,on=mainDestination)
            df2=df2[df2['oMeasure'] >= 0]
            df2=df2[df2['dMeasure'] >= 0]
            df2 = df2[(~df2['CPC'].isna())]
            orgTop2 = np.percentile(df2['oMeasure'],percentile)
            destTop2 = np.percentile(df2['dMeasure'],percentile)
            orgBot2 = np.percentile(df2['oMeasure'],100-percentile)
            destBot2 = np.percentile(df2['dMeasure'],100-percentile)
            topdf2 = df2[df2['oMeasure'] <= orgTop2]
            topdf2 = topdf2[topdf2['dMeasure'] <= destTop2]
            botdf2 = df2[df2['oMeasure'] >= orgBot2]
            botdf2 = botdf2[botdf2['dMeasure'] >= destBot2]
            
            
            #Adding random from existing smaple
            arrtop2 = topdf2['CPC'].values
            arrbot2 = botdf2['CPC'].values
            if(boolSaveDFs):
                topdf2.to_csv("topdf2.csv",index=False)
                botdf2.to_csv("botdf2.csv",index=False)
                np.savetxt('arrtop2.txt',arrtop2)
                np.savetxt('arrbot2.txt',arrbot2)
            arrtop2,arrbot2 = self.equalArray(arrtop2,arrbot2)
            count+=1

        if(measurement3 is not None):
            df3 = df.copy()
            meas3df = measurementdf[[measurementOrigin,measurement3]].copy()
            meas3df.rename(columns={measurementOrigin:mainOrigin, measurement3:'oMeasure'},inplace=True)
            df3 = pd.merge(df3,meas3df,on=mainOrigin)
            meas3df.rename(columns={mainOrigin:mainDestination,'oMeasure':'dMeasure'}, inplace=True)
            df3 = pd.merge(df3,meas3df,on=mainDestination)
            df3 =df3[df3['oMeasure'] >= 0]
            df3 =df3[df3['dMeasure'] >= 0]
            df3 = df3[(~df3['CPC'].isna())]
            orgTop3 = np.percentile(df3['oMeasure'],percentile)
            destTop3 = np.percentile(df3['dMeasure'],percentile)
            orgBot3 = np.percentile(df3['oMeasure'],100-percentile)
            destBot3 = np.percentile(df3['dMeasure'],100-percentile)
            topdf3 = df3[df3['oMeasure'] <= orgTop3]
            topdf3 = topdf3[topdf3['dMeasure'] <= destTop3]
            botdf3 = df3[df3['oMeasure'] >= orgBot3]
            botdf3 = botdf3[botdf3['dMeasure'] >= destBot3]
            
            #Adding random from existing sample
            arrtop3 = topdf3['CPC'].values
            arrbot3 = botdf3['CPC'].values
            if(boolSaveDFs):
                topdf3.to_csv("topdf3.csv",index=False)
                botdf3.to_csv("botdf3.csv",index=False)
                np.savetxt('arrtop3.txt',arrtop3)
                np.savetxt('arrbot3.txt',arrbot3)
            arrtop3,arrbot3 = self.equalArray(arrtop3,arrbot3)
            count+=1



        if(measurement4 is not None):
            df4 = df.copy()
            meas4df = measurementdf[[measurementOrigin,measurement4]].copy()
            meas4df.rename(columns={measurementOrigin:mainOrigin, measurement4:'oMeasure'},inplace=True)
            df4 = pd.merge(df4,meas4df,on=mainOrigin)
            meas4df.rename(columns={mainOrigin:mainDestination, 'oMeasure':'dMeasure'},inplace=True)
            df4 = pd.merge(df4,meas4df,on=mainDestination)
            df4 = df4[df4['oMeasure'] >= 0]
            df4 = df4[df4['dMeasure'] >= 0]
            df4 = df4[(~df4['CPC'].isna())]
            orgTop4 = np.percentile(df4['oMeasure'],percentile)
            destTop4 = np.percentile(df4['dMeasure'],percentile)
            orgBot4 = np.percentile(df4['oMeasure'],100-percentile)
            destBot4 = np.percentile(df4['dMeasure'],100-percentile)
            topdf4 = df4[df4['oMeasure'] <= orgTop4]
            topdf4 = topdf4[topdf4['dMeasure'] <= destTop4]
            botdf4 = df4[df4['oMeasure'] >= orgBot4]
            botdf4 = botdf4[botdf4['dMeasure'] >= destBot4]
            
            arrtop4 = topdf4['CPC'].values
            arrbot4 = botdf4['CPC'].values

            if(boolSaveDFs):
                topdf4.to_csv("topdf4.csv",index=False)
                botdf4.to_csv("botdf4.csv",index=False)
                np.savetxt('arrtop4.txt',arrtop4)
                np.savetxt('arrbot4.txt',arrbot4)
            arrtop4,arrbot4 = self.equalArray(arrtop4,arrbot4)
            count+=1

        if(measurement5 is not None):
            df5 = df.copy()
            meas5df = measurementdf[[measurementOrigin,measurement5]].copy()
            meas5df.rename(columns={measurementOrigin:mainOrigin, measurement5:'oMeasure'},inplace=True)
            df5 = pd.merge(df5,meas5df,on=mainOrigin)
            meas5df.rename(columns={mainOrigin:mainDestination, 'oMeasure':'dMeasure'},inplace=True)
            df5 = pd.merge(df5,meas5df,on=mainDestination)
            df5 = df5[df5['oMeasure'] >= 0]
            df5 = df5[df5['dMeasure'] >= 0]
            df5 = df5[(~df5['CPC'].isna())]
            orgTop5 = np.percentile(df5['oMeasure'],percentile)
            destTop5 = np.percentile(df5['dMeasure'],percentile)
            orgBot5 = np.percentile(df5['oMeasure'],100-percentile)
            destBot5 = np.percentile(df5['dMeasure'],100-percentile)
            topdf5 = df5[df5['oMeasure'] <= orgTop5]
            topdf5 = topdf5[topdf5['dMeasure'] <= destTop5]
            botdf5 = df5[df5['oMeasure'] >= orgBot5]
            botdf5 = botdf5[botdf5['dMeasure'] >= destBot5]

            arrtop5 = topdf5['CPC'].values
            arrbot5 = botdf5['CPC'].values

            if(boolSaveDFs):
                topdf5.to_csv("topdf5.csv",index=False)
                botdf5.to_csv("botdf5.csv",index=False)
                np.savetxt('arrtop5.txt',arrtop5)
                np.savetxt('arrbot5.txt',arrbot5)
            arrtop5,arrbot5 = self.equalArray(arrtop5,arrbot5)
            count+=1
        
        orgTop = np.percentile(df1['oMeasure'],percentile)
        destTop = np.percentile(df1['dMeasure'],percentile)
        orgBot = np.percentile(df1['oMeasure'],100-percentile)
        destBot = np.percentile(df1['dMeasure'],100-percentile)
        topdf = df1[df1['oMeasure'] <= orgTop]
        topdf = topdf[topdf['dMeasure'] <= destTop]
        botdf = df1[df1['oMeasure'] >= orgBot]
        botdf = botdf[botdf['dMeasure'] >= destBot]
        
        arrtop1 = topdf['CPC'].values
        arrbot1 = botdf['CPC'].values
        if(boolSaveDFs):
            topdf.to_csv("topdf.csv",index=False)
            botdf.to_csv("botdf.csv",index=False)
            np.savetxt('arrtop1.txt',arrtop1)
            np.savetxt('arrbot1.txt',arrbot1)
        arrtop1,arrbot1 = self.equalArray(arrtop1,arrbot1)



        js_div,kldiv = self.KLDiva(arrtop1,arrbot1)
        print("Jensen-Shannon Divergence: ", js_div)
        print("KL Divergence: ", kldiv)


        fig = make_subplots(rows = 1, cols = count)
        fig.update_layout(
            
            title = f"Top and Bottom {percentile} Percent for " + measurement
            #title="CPC Spread Across Measurements"
        )
        
        fig.add_trace(go.Box(name = "Richer Areas " + measurement, y=arrtop1, boxmean='sd'),row=1,col=1)
        fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement, y=arrbot1, boxmean='sd'),row=1,col=1)
        drawn = 1
        if(measurement2 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement2, y=arrtop2, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement2, y=arrbot2, boxmean='sd'),row=1,col=drawn)
            title = measurement2 
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div2,kldiv2 = self.KLDiva(arrtop2,arrbot2)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div2))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv2))

        if(measurement3 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement3, y=arrtop3, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement3, y=arrbot3, boxmean='sd'),row=1,col=drawn)
            title = measurement3
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div3,kldiv3 = self.KLDiva(arrtop3,arrbot3)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div3))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv3))

        if(measurement4 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement4, y=arrtop4, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement4, y=arrbot4, boxmean='sd'),row=1,col=drawn)
            title = measurement4
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div4,kldiv4 = self.KLDiva(arrtop4,arrbot4)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div4))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv4))
        
        if(measurement5 is not None):
            drawn+=1
            fig.add_trace(go.Box(name = "Richer Areas " + measurement5, y=arrtop5, boxmean='sd'),row=1,col=drawn)
            fig.add_trace(go.Box(name = "Less Wealthy Areas " + measurement5, y=arrbot5, boxmean='sd'),row=1,col=drawn)
            title = measurement5 
            fig.update_xaxes(title_text=title, row=1, col=drawn)
            js_div5,kldiv5 = self.KLDiva(arrtop5,arrbot5)
            print("Jensen-Shannon Divergence" + str(drawn) +" : " + str(js_div5))
            print("KL Divergence " + str(drawn) +" : " + str(kldiv5))

        title = measurement 
        fig.update_xaxes(title_text=title, row=1, col=1)
        

        fig.show()

        print("SVI Low End CPC Mean:", np.mean(topdf['CPC']))
        print("SVI Low End CPC Standard Deviation:", np.std(topdf['CPC']))
        print("SVI High End CPC Mean:", np.mean(botdf['CPC']))
        print("SVI High End CPC Standard Deviation:", np.std(botdf['CPC']))
        tstat, pval = stats.ttest_ind(topdf['CPC'],botdf['CPC'])
        print("T-statistic: ", tstat)
        print("p-value: ", pval)
            
        plt.show()
    
    def drawPlot(self, data):
        patterns = ["/", "\\", ".", "o", "+"]
        fig, ax = plt.subplots(figsize=(10, 6))

        print('drawing')
        boxes_bot = []
        for i in range(int(len(data)/2)):
            box_bot = ax.boxplot(data[i], positions=[i+1], widths=0.8, patch_artist=True, boxprops=dict(facecolor='white', edgecolor='blue', hatch=patterns[i % 5]), medianprops=dict(color='blue'), flierprops=dict(markeredgecolor='blue'), whiskerprops=dict(color='blue'), capprops=dict(color='blue'), showfliers=True)
            boxes_bot.append(box_bot)
        print('done1')
        boxes_top = []
        for i in range(int(len(data)/2 + 2 ), int(len(data) + 1)):
            print(i)
            box_top = ax.boxplot(data[i-2], positions=[i+1], widths=0.8, patch_artist=True, boxprops=dict(facecolor='white', edgecolor='black', hatch=patterns[i % 7]), medianprops=dict(color='black'), flierprops=dict(markeredgecolor='black'), whiskerprops=dict(color='black'), capprops=dict(color='black'), showfliers=True)
            boxes_top.append(box_top)
        
        ticks = []
        for i in range(len(data) + 2):
            ticks.append("")
        ax.set_xticks(range(1,len(data)+3))
        ax.set_xticklabels(ticks)
        ax.set_xlim(0, len(data)+3)

        ax.set_ylabel("CPC")
        fig.subplots_adjust(left=0.1, right=0.8)
        plt.show()


def Counterfactual_fairness(normF, data, sensitive_attribute, feature, epsilon, geoid="FIPS"):
  '''
  This function select a random pair from the normF (output csv file from Daniel's package), find a similar pair
  that is based on the feature. Upon finding all the possible pairs, it will then find the two pairs that has
  the max and min sensitive_attribute value in all_possible_destination and store it into pair2 
  for the comparison later. 
  Next up, function will use the absolute number of pair1's sensitive attribute minus each pair in the pair2 (2 pairs in pair2),  
  whichever has the higher difference will be the pair to compare with. Then return the difference of CPC between pair1 and pair2

  @param
  normF: Output csv file from Daniel's package, it contains the origin, destination, and CPC for the pair
  data: Census csv file that contain the information of the area
  sensitve_attribute: A string that specify the sensitive attribute column in 'data' that you are trying to exam
  feature: A string that specify the feature from 'data'
  epsilon: A float number to specify the percentage you are looking for
  geoid: A string to specify the column name for the geoid, default = FIPS

  @return
  float 

  '''

  # Randomly select one OD pair 
  indiv = normF.sample(n=1)
  # Store the origin and destination FIPS into separate variables
  indiv_o = indiv.origin
  indiv_d = indiv.destination
  print(f'Performed CF fairness on: Origin: {int(indiv_o)}, Destination: {int(indiv_d)}')

  # Search both of the origin and destination in the dataset
  # for the feature
  o = data.loc[(data[geoid] == int(indiv_o))]
  d = data.loc[(data[geoid] == int(indiv_d))]
  
  # Concat them together as pair1
  pair = [o, d]
  pair1 = pd.concat(pair)

  # Grab the feature of pair1
  pair1_feature = pair1[feature]

  # Finding similar pair, pair2
  # Eliminate pair1 from the dataset to prevent duplicate
  geoid_values = pair1[geoid].values
  geoid_value_1 = geoid_values[0]
  geoid_value_2 = geoid_values[1] if len(geoid_values) > 1 else None
  sub_data = data.loc[~data[geoid].isin([geoid_value_1, geoid_value_2])]

  # Calculate the range of value for searching
  if epsilon == 1:
    # This line prevents o_min_value = 0 when epsilon = 1
    # e.g.  epsilon = 1, pair1_feature.iloc[0] = 4000
    #       o_min_value = 4000 - (1 * 4000) -> 0
    #       value should be the same, 4000 in this case
    all_possible_destinations = sub_data.loc[(sub_data[feature] == pair1_feature.iloc[0]) & (sub_data[feature] == pair1_feature.iloc[1])]
    if all_possible_destinations.empty:
      print('No similar pair, please increase epsilon or try again for a different random pair')
      return None
  else: 
    # In case if there is only 1 element in pair1_feature
    if len(pair1_feature) >= 2:
      o_min_value = pair1_feature.iloc[0] - (epsilon * pair1_feature.iloc[0])
      d_min_value = pair1_feature.iloc[1] - (epsilon * pair1_feature.iloc[1])
      o_max_value = pair1_feature.iloc[0] + (epsilon * pair1_feature.iloc[0])
      d_max_value = pair1_feature.iloc[1] + (epsilon * pair1_feature.iloc[1])
    else: 
      o_min_value = pair1_feature.iloc[0] - (epsilon * pair1_feature.iloc[0])
      d_min_value = pair1_feature.iloc[0] - (epsilon * pair1_feature.iloc[0])
      o_max_value = pair1_feature.iloc[0] + (epsilon * pair1_feature.iloc[0])
      d_max_value = pair1_feature.iloc[0] + (epsilon * pair1_feature.iloc[0])
    # Find all the similar destination that are between the range of values from the above calculation
    all_possible_destinations = sub_data[sub_data[feature].between(o_min_value, o_max_value) & sub_data[feature].between(d_min_value, d_max_value)]
    if all_possible_destinations.empty:
      print('No similar pair, please increase epsilon or try again for a different random pair')
      return None
  
  # Get all the OD pairs and store it in pair2
  pair2 = pd.DataFrame()
  sa = []
  for i in range(len(all_possible_destinations)):
    # Loacate all the pairs that match the possible destination fips 
    temp_df = normF.loc[(normF['origin'] == all_possible_destinations[geoid].iloc[i]) | (normF['destination'] == all_possible_destinations[geoid].iloc[i])]
    pair2 = pd.concat([pair2, temp_df])

  # Loop to get the total value of sensitive attribute for origin and destination 
  # from the normF for the comparison later
  for j in range(len(pair2)):
    o_sa = pair2['origin'].iloc[j]
    d_sa = pair2['destination'].iloc[j]

    o_sa = sub_data.loc[(sub_data[geoid] == o_sa)]
    d_sa = sub_data.loc[(sub_data[geoid] == d_sa)]

    # In case of the empty sensitive attribute
    if o_sa.empty and d_sa.empty:
      # If both origin & destination's SA is empty
      # total will be 0
      total = 0
    elif o_sa.empty:
      # If origin SA is empty -> use destination's SA value
      total = d_sa[sensitive_attribute].item()
    elif d_sa.empty:
      # If destination SA is empty -> use origin's SA value
      total = o_sa[sensitive_attribute].item()
    else:
      # Get the total value if both are not empty
      o_sa = o_sa[sensitive_attribute].item()
      d_sa = d_sa[sensitive_attribute].item()
      total = o_sa + d_sa
    sa.append(total)

  pair2[sensitive_attribute] = sa
  print(f'Total number of similar pair: {len(pair2)}')

  # Sensitive attribute calculation
  pair2_max = pair2[sensitive_attribute].max()
  pair2_min = pair2[sensitive_attribute].min()
  print('Among all the possible similar pair:')
  print(f'Minimum sensitive attribute value: {pair2_min}')
  print(f'Maximum sensitive attribute value: {pair2_max}')

  # Get the 2 pairs
  pair2 = pair2[pair2[sensitive_attribute].isin([pair2_max, pair2_min])]
  
  # Compare
  pair1_cpc = indiv['CPC']
  pair2_cpc = pair2['CPC']

  # Pair1's sensitive attribute value
  pair1_sa = pair1[sensitive_attribute].iloc[0].item() + pair1[sensitive_attribute].iloc[1].item()
  
  # Determine which one of the pair in pair2 to compare with pair1
  sa_compare_result1 = abs(pair1_sa - pair2_max)
  sa_compare_result2 = abs(pair1_sa - pair2_min)
  determine_pair2 = max(abs(pair1_sa - pair2_max), abs(pair1_sa - pair2_min))

  if determine_pair2 == sa_compare_result1:
    result = abs(pair1_cpc - pair2_cpc.iloc[0])
  elif determine_pair2 == sa_compare_result2:
    result = abs(pair1_cpc - pair2_cpc.iloc[1])
  else:
    result = 1

  return float(result)
